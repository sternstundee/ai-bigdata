import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve
)
import matplotlib.pyplot as plt
import numpy as np
import os


# ---------------------- åŸºç¡€é…ç½®ï¼ˆå¤ç”¨ä½ çš„æ•°æ®è·¯å¾„å’Œåˆ—åï¼‰----------------------
file_path = "D:/pythonstudy/aibdlesson/labor7/loan_YN.csv"
target_col = "loan"  # ç›®æ ‡åˆ—ï¼ˆè´·æ¬¾çŠ¶æ€ï¼‰
feature_cols = ["age", "income", "guding", "VIP"]  # ç‰¹å¾åˆ—ï¼ˆ4ä¸ªè¾“å…¥ï¼‰


# ---------------------- 1. æ•°æ®è¯»å–ä¸é¢„å¤„ç†ï¼ˆå¤ç”¨ç¼–ç é€»è¾‘ï¼‰----------------------
def read_data_with_encoding(file_path):
    if file_path.endswith(".csv"):
        try:
            return pd.read_csv(file_path, encoding="gbk")
        except UnicodeDecodeError:
            return pd.read_csv(file_path, encoding="utf-8-sig")


df = read_data_with_encoding(file_path)
X = df[feature_cols]  # ä»…ä¿ç•™4ä¸ªç‰¹å¾åˆ—
y = df[target_col]  # ç›®æ ‡åˆ—

# One-Hotç¼–ç ï¼ˆåˆ†ç±»ç‰¹å¾guding/VIPï¼‰
categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()
numeric_cols = X.select_dtypes(exclude=["object"]).columns.tolist()

vec = DictVectorizer(sparse=False)
X_categorical_encoded = vec.fit_transform(X[categorical_cols].to_dict("records"))
categorical_feature_names = vec.get_feature_names()

# åˆå¹¶ç‰¹å¾
X_numeric = X[numeric_cols].values
X_encoded = np.hstack([X_numeric, X_categorical_encoded])
all_feature_names = numeric_cols + categorical_feature_names

print("=" * 80)
print("ğŸ“Š æ€è€ƒé¢˜ï¼šå†³ç­–æ ‘æ¨¡å‹ä¼˜åŒ–ä¸æ³›åŒ–èƒ½åŠ›éªŒè¯")
print(f"æ•°æ®è§„æ¨¡ï¼š{X_encoded.shape[0]} æ ·æœ¬ï¼Œ{X_encoded.shape[1]} ç‰¹å¾")
print(f"ç‰¹å¾åç§°ï¼š{all_feature_names}")
print("=" * 80)

# ---------------------- 2. æ ¸å¿ƒæ­¥éª¤1ï¼šåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰----------------------
# è®­ç»ƒé›†70%ï¼ˆç”¨äºå»ºæ¨¡ï¼‰ï¼Œæµ‹è¯•é›†30%ï¼ˆç”¨äºéªŒè¯çœŸå®æ€§èƒ½ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.3, random_state=42, stratify=y  # stratify=yï¼šä¿æŒç±»åˆ«åˆ†å¸ƒä¸€è‡´
)

print("âœ… è®­ç»ƒé›†/æµ‹è¯•é›†åˆ’åˆ†å®Œæˆï¼š")
print(f"è®­ç»ƒé›†ï¼š{X_train.shape[0]} æ ·æœ¬ï¼ˆ70%ï¼‰")
print(f"æµ‹è¯•é›†ï¼š{X_test.shape[0]} æ ·æœ¬ï¼ˆ30%ï¼‰")
print(f"è®­ç»ƒé›†è´·æ¬¾çŠ¶æ€åˆ†å¸ƒï¼š{y_train.value_counts().to_dict()}")
print(f"æµ‹è¯•é›†è´·æ¬¾çŠ¶æ€åˆ†å¸ƒï¼š{y_test.value_counts().to_dict()}")
print("=" * 80)

# ---------------------- 3. æ ¸å¿ƒæ­¥éª¤2ï¼šå†³ç­–æ ‘å‚æ•°ä¼˜åŒ–ï¼ˆå¯¹æ¯”ä¸åŒæ·±åº¦çš„æ•ˆæœï¼‰----------------------
# æµ‹è¯•ä¸åŒmax_depthï¼ˆæ ‘æ·±åº¦ï¼‰ï¼Œæ‰¾åˆ°æœ€ä¼˜å‚æ•°
max_depths = [1, 2, 3, 4, 5, 6, 7, 8]  # å¾…æµ‹è¯•çš„æ·±åº¦
train_acc_list = []  # è®­ç»ƒé›†å‡†ç¡®ç‡
test_acc_list = []  # æµ‹è¯•é›†å‡†ç¡®ç‡
auc_list = []  # æµ‹è¯•é›†AUCå€¼ï¼ˆæ›´å…¨é¢çš„åˆ†ç±»è¯„ä¼°æŒ‡æ ‡ï¼‰

for depth in max_depths:
    # è®­ç»ƒå†³ç­–æ ‘
    dt = DecisionTreeClassifier(
        max_depth=depth,
        random_state=42,
        criterion="gini"
    )
    dt.fit(X_train, y_train)

    # é¢„æµ‹å¹¶è®¡ç®—æŒ‡æ ‡
    y_train_pred = dt.predict(X_train)
    y_test_pred = dt.predict(X_test)
    y_test_prob = dt.predict_proba(X_test)[:, 1]  # æ­£ç±»æ¦‚ç‡ï¼ˆç”¨äºAUCè®¡ç®—ï¼‰

    # ä¿å­˜æŒ‡æ ‡
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    auc = roc_auc_score(y_test, y_test_prob) if len(np.unique(y_test)) > 1 else 0

    train_acc_list.append(train_acc)
    test_acc_list.append(test_acc)
    auc_list.append(auc)

    print(f"ğŸŒ³ æ ‘æ·±åº¦={depth}ï¼š")
    print(f"  è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š{train_acc:.3f} | æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š{test_acc:.3f} | æµ‹è¯•é›†AUCï¼š{auc:.3f}")

print("=" * 80)

# ---------------------- 4. ç»“æœå¯è§†åŒ–ï¼šå‚æ•°ä¼˜åŒ–å¯¹æ¯”å›¾ ----------------------
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.figure(figsize=(12, 6))

# ç»˜åˆ¶å‡†ç¡®ç‡å¯¹æ¯”
plt.plot(max_depths, train_acc_list, marker="o", label="è®­ç»ƒé›†å‡†ç¡®ç‡", linewidth=2)
plt.plot(max_depths, test_acc_list, marker="s", label="æµ‹è¯•é›†å‡†ç¡®ç‡", linewidth=2)
plt.plot(max_depths, auc_list, marker="^", label="æµ‹è¯•é›†AUC", linewidth=2)

# æ ‡æ³¨æœ€ä¼˜å‚æ•°ï¼ˆæµ‹è¯•é›†å‡†ç¡®ç‡æœ€é«˜çš„æ·±åº¦ï¼‰
best_depth = max_depths[test_acc_list.index(max(test_acc_list))]
plt.scatter(best_depth, max(test_acc_list), color="red", s=100, zorder=5)
plt.annotate(
    f"æœ€ä¼˜æ·±åº¦={best_depth}\nå‡†ç¡®ç‡={max(test_acc_list):.3f}",
    xy=(best_depth, max(test_acc_list)),
    xytext=(best_depth + 0.5, max(test_acc_list) - 0.05),
    arrowprops=dict(arrowstyle="->", color="red")
)

plt.xlabel("å†³ç­–æ ‘æœ€å¤§æ·±åº¦ï¼ˆmax_depthï¼‰", fontsize=12)
plt.ylabel("æŒ‡æ ‡å€¼ï¼ˆå‡†ç¡®ç‡/AUCï¼‰", fontsize=12)
plt.title("å†³ç­–æ ‘å‚æ•°ä¼˜åŒ–ï¼šä¸åŒæ·±åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“", fontsize=14, fontweight="bold")
plt.legend()
plt.grid(True, alpha=0.3)
plt.xticks(max_depths)
plt.tight_layout()
param_plot_path = "D:/pythonstudy/aibdlesson/labor7/decision_tree_param_optimization.png"
plt.savefig(param_plot_path, dpi=300)
plt.show()
print(f"ğŸ“¸ å‚æ•°ä¼˜åŒ–å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°ï¼š{param_plot_path}")
print("=" * 80)

# ---------------------- 5. æœ€ä¼˜æ¨¡å‹çš„è¯¦ç»†è¯„ä¼°ï¼ˆå®éªŒæŠ¥å‘Šæ ¸å¿ƒå†…å®¹ï¼‰----------------------
# åŸºäºæœ€ä¼˜æ·±åº¦è®­ç»ƒæœ€ç»ˆæ¨¡å‹
best_dt = DecisionTreeClassifier(
    max_depth=best_depth,
    random_state=42,
    criterion="gini"
)
best_dt.fit(X_train, y_train)

# è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
y_test_pred = best_dt.predict(X_test)
conf_matrix = confusion_matrix(y_test, y_test_pred)
class_report = classification_report(y_test, y_test_pred, output_dict=True)

print("ğŸ† æœ€ä¼˜å†³ç­–æ ‘æ¨¡å‹ï¼ˆmax_depth={}ï¼‰è¯¦ç»†è¯„ä¼°ï¼š".format(best_depth))
print("\n1. æ··æ·†çŸ©é˜µï¼ˆçœŸå®æ ‡ç­¾vsé¢„æµ‹æ ‡ç­¾ï¼‰ï¼š")
print(conf_matrix)
print("\n2. åˆ†ç±»æŠ¥å‘Šï¼ˆç²¾ç¡®ç‡/å¬å›ç‡/F1å€¼ï¼‰ï¼š")
print(classification_report(y_test, y_test_pred))

# æ··æ·†çŸ©é˜µå¯è§†åŒ–
plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, interpolation="nearest", cmap=plt.cm.Blues)
plt.title("æœ€ä¼˜æ¨¡å‹æ··æ·†çŸ©é˜µ", fontsize=14, fontweight="bold")
plt.colorbar()
classes = [str(cls) for cls in best_dt.classes_]
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes, fontsize=12)
plt.yticks(tick_marks, classes, fontsize=12)

# åœ¨æ··æ·†çŸ©é˜µä¸­æ·»åŠ æ•°å€¼æ ‡ç­¾
thresh = conf_matrix.max() / 2.
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        plt.text(j, i, format(conf_matrix[i, j], "d"),
                 horizontalalignment="center",
                 color="white" if conf_matrix[i, j] > thresh else "black")

plt.ylabel("çœŸå®æ ‡ç­¾", fontsize=12)
plt.xlabel("é¢„æµ‹æ ‡ç­¾", fontsize=12)
plt.tight_layout()
conf_matrix_path = "D:/pythonstudy/aibdlesson/labor7/confusion_matrix_best_model.png"
plt.savefig(conf_matrix_path, dpi=300)
plt.show()
print(f"ğŸ“¸ æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜åˆ°ï¼š{conf_matrix_path}")
print("=" * 80)

# ---------------------- 6. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆä¼˜åŒ–åæ¨¡å‹vsåŸå§‹æ¨¡å‹ï¼‰----------------------
# åŸå§‹æ¨¡å‹ï¼ˆæ·±åº¦=3ï¼‰çš„ç‰¹å¾é‡è¦æ€§
original_dt = DecisionTreeClassifier(max_depth=3, random_state=42)
original_dt.fit(X_encoded, y)
original_importance = pd.DataFrame({
    "ç‰¹å¾åç§°": all_feature_names,
    "åŸå§‹æ¨¡å‹é‡è¦æ€§": original_dt.feature_importances_
})

# æœ€ä¼˜æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
best_importance = pd.DataFrame({
    "ç‰¹å¾åç§°": all_feature_names,
    "æœ€ä¼˜æ¨¡å‹é‡è¦æ€§": best_dt.feature_importances_
})

# åˆå¹¶å¯¹æ¯”
importance_compare = pd.merge(original_importance, best_importance, on="ç‰¹å¾åç§°")
print("ğŸ“Š ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼ˆåŸå§‹æ¨¡å‹vsæœ€ä¼˜æ¨¡å‹ï¼‰ï¼š")
print(importance_compare.sort_values("æœ€ä¼˜æ¨¡å‹é‡è¦æ€§", ascending=False).to_string(index=False))

# å¯è§†åŒ–å¯¹æ¯”
plt.figure(figsize=(12, 6))
x = np.arange(len(all_feature_names))
width = 0.35

plt.bar(x - width / 2, importance_compare["åŸå§‹æ¨¡å‹é‡è¦æ€§"], width, label="åŸå§‹æ¨¡å‹ï¼ˆæ·±åº¦=3ï¼‰")
plt.bar(x + width / 2, importance_compare["æœ€ä¼˜æ¨¡å‹é‡è¦æ€§"], width, label="æœ€ä¼˜æ¨¡å‹ï¼ˆæ·±åº¦={}ï¼‰".format(best_depth))

plt.xlabel("ç‰¹å¾åç§°", fontsize=12)
plt.ylabel("ç‰¹å¾é‡è¦æ€§", fontsize=12)
plt.title("ç‰¹å¾é‡è¦æ€§å¯¹æ¯”ï¼šåŸå§‹æ¨¡å‹vsæœ€ä¼˜æ¨¡å‹", fontsize=14, fontweight="bold")
plt.xticks(x, all_feature_names, rotation=45)
plt.legend()
plt.tight_layout()
importance_compare_path = "D:/pythonstudy/aibdlesson/labor7/feature_importance_compare.png"
plt.savefig(importance_compare_path, dpi=300)
plt.show()
print(f"ğŸ“¸ ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°ï¼š{importance_compare_path}")
print("=" * 80)

# ---------------------- 7. ä¿å­˜æ€è€ƒé¢˜æ‰€æœ‰ç»“æœ ----------------------
# ä¿å­˜å‚æ•°ä¼˜åŒ–ç»“æœ
param_results = pd.DataFrame({
    "å†³ç­–æ ‘æ·±åº¦": max_depths,
    "è®­ç»ƒé›†å‡†ç¡®ç‡": train_acc_list,
    "æµ‹è¯•é›†å‡†ç¡®ç‡": test_acc_list,
    "æµ‹è¯•é›†AUC": auc_list
})
param_results_path = "D:/pythonstudy/aibdlesson/labor7/param_optimization_results.csv"
param_results.to_csv(param_results_path, index=False, encoding="utf-8-sig")

# ä¿å­˜æœ€ä¼˜æ¨¡å‹è¯„ä¼°ç»“æœ
eval_results = pd.DataFrame({
    "æŒ‡æ ‡": ["å‡†ç¡®ç‡", "ç²¾ç¡®ç‡", "å¬å›ç‡", "F1å€¼", "AUC"],
    "æ•°å€¼": [
        test_acc_list[test_acc_list.index(max(test_acc_list))],
        class_report[list(class_report.keys())[1]]["precision"],
        class_report[list(class_report.keys())[1]]["recall"],
        class_report[list(class_report.keys())[1]]["f1-score"],
        max(auc_list)
    ]
})
eval_results_path = "D:/pythonstudy/aibdlesson/labor7/best_model_evaluation.csv"
eval_results.to_csv(eval_results_path, index=False, encoding="utf-8-sig")

print("ğŸ’¾ æ€è€ƒé¢˜ç»“æœå·²å…¨éƒ¨ä¿å­˜ï¼š")
print(f"  1. å‚æ•°ä¼˜åŒ–å¯¹æ¯”å›¾ï¼š{param_plot_path}")
print(f"  2. æ··æ·†çŸ©é˜µå›¾ï¼š{conf_matrix_path}")
print(f"  3. ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾ï¼š{importance_compare_path}")
print(f"  4. å‚æ•°ä¼˜åŒ–æ•°æ®ï¼š{param_results_path}")
print(f"  5. æœ€ä¼˜æ¨¡å‹è¯„ä¼°æ•°æ®ï¼š{eval_results_path}")
print("=" * 80)
print("ğŸ¯ æ€è€ƒé¢˜æ ¸å¿ƒç»“è®ºï¼ˆç›´æ¥å†™å…¥å®éªŒæŠ¥å‘Šï¼‰ï¼š")
print(f"  1. åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†åï¼Œæ¨¡å‹çš„çœŸå®æ€§èƒ½ï¼ˆæµ‹è¯•é›†å‡†ç¡®ç‡ï¼‰æ¯”è®­ç»ƒé›†å‡†ç¡®ç‡æ›´å¯é ï¼Œé¿å…äº†è¿‡æ‹Ÿåˆã€‚")
print(
    f"  2. å†³ç­–æ ‘æœ€ä¼˜æ·±åº¦ä¸º {best_depth}ï¼Œæ­¤æ—¶æµ‹è¯•é›†å‡†ç¡®ç‡æœ€é«˜ï¼ˆ{max(test_acc_list):.3f}ï¼‰ï¼Œæ·±åº¦è¿‡æ·±ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆè®­ç»ƒé›†å‡†ç¡®ç‡é«˜ä½†æµ‹è¯•é›†ä½ï¼‰ã€‚")
print(
    f"  3. æœ€ä¼˜æ¨¡å‹çš„æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡ï¼šç²¾ç¡®ç‡={class_report[list(class_report.keys())[1]]['precision']:.3f}ï¼Œå¬å›ç‡={class_report[list(class_report.keys())[1]]['recall']:.3f}ï¼ŒF1å€¼={class_report[list(class_report.keys())[1]]['f1-score']:.3f}ã€‚")
print(
    f"  4. å¯¹æ¯”åŸå§‹æ¨¡å‹ï¼Œæœ€ä¼˜æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§æ›´é›†ä¸­ï¼Œæ ¸å¿ƒå½±å“å› ç´ ï¼ˆå¦‚{importance_compare.sort_values('æœ€ä¼˜æ¨¡å‹é‡è¦æ€§', ascending=False).iloc[0]['ç‰¹å¾åç§°']}ï¼‰æ›´çªå‡ºã€‚")